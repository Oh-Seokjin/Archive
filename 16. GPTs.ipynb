{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a447ad3b1b468ebce43772cea33557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfdff5033e04f1690927706093d68f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 464, 3139,  286, 4969,  318]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Korea -> seoul, USA -> washington.dc, France -> paris\n",
    "prompt = \"The capital of Korea is\"\n",
    "\n",
    "encodings = tokenizer(prompt, return_tensors='pt')\n",
    "print(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  3139,   286,  4969,   318, 22372,    11,   543,   318,  1363,\n",
      "           284,   262,  1499,   338,  4387, 10368,   286,  3215,    12,  6286]])\n"
     ]
    }
   ],
   "source": [
    "# model.generate -> input ids를 넣으면 다음 토큰을 생성\n",
    "outputs = model.generate(max_length=20, **encodings)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The capital of Korea is Seoul, which is home to the country's largest concentration of foreign-born\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = ['korea', 'USA', 'France', 'Germany', 'Italia']\n",
    "\n",
    "for i in range(len(country_list)):\n",
    "    prompt = f'The capital of {country_list[i]} is'\n",
    "    encodings = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(max_length=20, **encodings)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt를 다음과 같이 기술하면 task description을 준 것으로 해석할 수 있음\n",
    "\n",
    "prompt = \"\"\"\\\n",
    "    Name the capital city for a given country.\n",
    "    The capital of Korea is\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    nation = input(\"국가를 입력: \")\n",
    "    if not nation.strip():\n",
    "        break\n",
    "    prompt = f\"\"\"\\\n",
    "    Name the capital city for a given country.\n",
    "    The capital of {nation.strip()} is\\\n",
    "    \"\"\"\n",
    "\n",
    "    encodings = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(max_length=20, **encodings)\n",
    "    print(tokenizer.decode(outputs[0]), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc8214fa935b74cd2993506950db861c39f3b536af543597dc14393642024e69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
