{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Language Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어모델은 단어 시퀀스에 확률을 할당하는 일  \n",
    "즉, 가장 자연스러운 문장 생성이 목적  \n",
    "BERT이전까지는 다음 단어를 예측하는 방식이 주류  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">P(\"나는 버스를 탔다\") > P(\"나는 버스를 태웠다\")  \n",
    "\n",
    ">비행기를 타려고 공항에 갔는데 지각을 하는 바람에 비행기를 [...]  \n",
    ">놓쳤다!\n",
    "\n",
    "가장 확률이 높은 단어를 선택하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 내 다음 단어 예측 시 사용되는조건부 확률의 Chain Rule  \n",
    "> P(a, b, c, d) = P(a) P(b|a) P(c|a, b) P(d|a, b, c)  \n",
    "\n",
    "- Sparsity Problem  \n",
    "SLM은 횟수를 카운트하여 확률적 방법으로 예측을 진행함  \n",
    "하지만 그 조합이 존재하지 않을 경우 확률이 0이되고, 문장이 길어지면 정확히 일치하는 시퀀스가 없을 확률이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전의 Sparsity Problem 해결을 위해, 전체 시퀀스의 조건부 확률을 구하지 않고, N개 단어만 고려함  \n",
    "Unigram, Bigrams, Trigrams, 4-grams ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한계\n",
    "  1. 제한된 정보만 사용하게 되어 문맥이 급격하게 바뀔 경우 정확하지 않을 수 있음  \n",
    "  2. 개선되긴 했지만, 여전히 Sparsity Problem 존재함.  \n",
    "  3. n을 크게 선택하면 Sparsity Problem이 심해지고, n을 작게 선택하면 real world와 동떨어진 결과를 얻을 수 있음 = Trade Off가 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
