{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oh-Seokjin/MRC_goorm/blob/main/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Korean MRC Baseline\n",
        "\n",
        "## Dependency\n",
        "다음과 같은 라이브러리를 사용한다.\n",
        "- [Konlpy](https://konlpy.org/ko/latest/index.html): 파이썬 한국어 NLP 처리기\n",
        "- [Mecab-korean](https://bitbucket.org/eunjeon/mecab-ko-dic/src): 한국어 형태소 분석기"
      ],
      "metadata": {
        "id": "OASO4zkA1ORR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! apt-get install -y openjdk-8-jdk python3-dev\n",
        "! pip install konlpy \"tweepy<4.0.0\"\n",
        "! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T08:23:53.548281Z",
          "iopub.execute_input": "2022-10-04T08:23:53.548910Z",
          "iopub.status.idle": "2022-10-04T08:24:06.698515Z",
          "shell.execute_reply.started": "2022-10-04T08:23:53.548827Z",
          "shell.execute_reply": "2022-10-04T08:24:06.697324Z"
        },
        "trusted": true,
        "id": "p9SwvQ3S1ORU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 구성\n",
        "현재 JSON 데이터를 볼 수 있는 클래스를 하나 작성하자."
      ],
      "metadata": {
        "id": "R0NCVddr1ORW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QnA에는 ELECTRA 많이 쓰는듯\n",
        "\n",
        "# Question기준으로 index나누어주는 것이 편리함\n",
        "# 0, 0, 0 question-context-paragraph 등으로 찾을 수 있도록\n",
        "\n",
        "# 사용하는 pretrained model 탐색\n",
        "\n",
        "# AI Hub 데이터를 테스트 데이터로 사용\n",
        "# 제출 시 짧을수록 유리(문단으로 낼 바에 Blank로 내는 것이 점수 높음)\n",
        "\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import json\n",
        "import random\n",
        "\n",
        "class KoMRC:\n",
        "    def __init__(self, data, indices: List[Tuple[int, int, int]]):\n",
        "        self._data = data\n",
        "        self._indices = indices\n",
        "\n",
        "    # Json을 불러오는 메소드\n",
        "    @classmethod\n",
        "    def load(cls, file_path: str):\n",
        "        with open(file_path, 'r', encoding='utf-8') as fd:\n",
        "            data = json.load(fd)\n",
        "\n",
        "        indices = []\n",
        "        for d_id, document in enumerate(data['data']):\n",
        "            for p_id, paragraph in enumerate(document['paragraphs']):\n",
        "                for q_id, _ in enumerate(paragraph['qas']):\n",
        "                    indices.append((d_id, p_id, q_id))\n",
        "        \n",
        "        return cls(data, indices)\n",
        "\n",
        "    # 데이터 셋을 잘라내는 메소드\n",
        "    @classmethod\n",
        "    def split(cls, dataset, eval_ratio: float=.1, seed=42):\n",
        "        indices = list(dataset._indices)\n",
        "        random.seed(seed)\n",
        "        random.shuffle(indices)\n",
        "        train_indices = indices[int(len(indices) * eval_ratio):]\n",
        "        eval_indices = indices[:int(len(indices) * eval_ratio)]\n",
        "\n",
        "        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
        "        d_id, p_id, q_id = self._indices[index]\n",
        "        paragraph = self._data['data'][d_id]['paragraphs'][p_id]\n",
        "\n",
        "        context = paragraph['context']\n",
        "        qa = paragraph['qas'][q_id]\n",
        "\n",
        "        guid = qa['guid']\n",
        "        question = qa['question']\n",
        "        answers = qa['answers']\n",
        "\n",
        "        return {\n",
        "            'guid': guid,\n",
        "            'context': context,\n",
        "            'question': question,\n",
        "            'answers': answers\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._indices)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "l218Dreo1ORW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`load` 메소드를 이용해서 Json 데이터를 불러올 수 있다."
      ],
      "metadata": {
        "id": "lVYb7-IP1ORX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqCNc3El2Ssg",
        "outputId": "85ea99f9-ad6c-46bd-eb5b-4516d69c21b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = KoMRC.load('/content/drive/MyDrive/Goorm/project02/data/train.json')\n",
        "print(\"Number of Samples:\", len(dataset))\n",
        "print(dataset[10])"
      ],
      "metadata": {
        "trusted": true,
        "id": "BV_zAxwc1ORX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0a83d4-58c2-40e8-e132-72fa6fac0e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Samples: 12037\n",
            "{'guid': 'f7c95fd5471a4e3eb8f25ffb2741f60f', 'context': '“제트기가 음속을 돌파하려면 비행기의 모든 소재가 다 바뀌어야 한다. 재료공학부터 기초물리, 화학까지 첨단기술이 총동원돼야 한다. 설계도는 물론이고 엔진과 부품을 완전히 바꿔야 한다.”6일 경기도 용인 삼성인력개발원에 모인 200여 삼성그룹 임원들은 이건희 회장이 7년 전 강조한 ‘마하경영’의 의미를 되새겼다. 김동재 연세대 국제학대학원 교수의 마하경영 강연에 이어 분임조별 토론이 벌어졌다. 이틀 동안 진행된 교육에서 강연과 토론에 할애된 시간만 20시간이 넘었다. 삼성이 매년 2월 진행하는 임원진 교육 현장이다. 삼성은 그룹 내 2000여명의 임원을 10개 조로 나눠 2월 한 달 동안 1박2일간 합숙 교육한다.올해는 한계돌파를 위한 마하경영에 임원 교육의 초점이 맞춰졌다. 지난 연말 열린 사장단 세미나와 지난달 신임 임원 교육에 이어 2월 기존 임원진 교육에서도 공통된 주제는 마하경영이다. 하나의 방향을 정하면 그대로 실행에 옮기는 삼성의 스파르타식 교육의 한 단면이다.이날 강연을 들은 한 임원은 “그룹 차원에서 설정한 방향과 가치를 공유하며 정신을 무장하는 자리”라며 “현업에서 이를 어떻게 실천할 것인지 고민해보는 시간을 가졌다”고 말했다. 그는 “삼성의 힘은 결국 이런 교육에서 나온다”고 덧붙였다. 이날 마하경영을 설명한 김 교수는 지난달 신임 임원 교육에서도 강연자로 나섰다. ‘마하(Mach)’는 음속 제트기의 속도를 측정하는 단위다. 그는 “음속을 돌파하려면 부분 개보수가 아니라 자기부정부터 시작해 행동부터 생각까지 근본적으로 바뀌어야 한다”고 강조했다.이 회장은 2006년 당시 전자 계열사 사장들에게 “삼성의 약점을 보완하고 성장하려면 마하1이나 2가 아니라 마하3은 돼야 한다”며 “현재 삼성은 음속 이하의 수준인 만큼 진정한 글로벌 선진 기업이 되려면 더 분발해야 한다”고 했다.1993년 신경영 선언 당시 “보잉747은 이륙할 때 몇분 만에 1만ｍ까지 올라가야 하는데 중간에 멈추면 그대로 추락하거나 공중폭발을 하고 만다”는 ‘비행기 이륙’에서 ‘제트기 음속’으로 진화한 개념이다. 올해 다시 등장한 마하경영은 속도에서 가치 향상으로의 지속적인 혁신에 중점을 뒀다. 1등을 빨리 따라잡아야 한다는 추격자에서 이젠 세계 1등 기업에 맞는 체질로의 변신을 요구하고 있는 것이다. 마하경영과 함께 2등을 확실히 따돌릴 정도의 차이를 벌릴 때까지 안심해서는 안 된다는 ‘초격차’와 변화의 주도권을 잡기 위한 ‘한계 돌파’도 화두로 제시됐다. 올해 승진한 300여 신임 임원들은 지난달 5박6일간 마하경영 교육과 함께 임원으로서의 기본소양과 리더십 교육을 받았다. 다른 기업들과 달리 삼성은 직원들 교육만큼 임원 교육 비중이 크다.신임임원 교육으로 입문한 이후엔 매년 1박2일간의 임원 교육에 참석해야 하고 부사장급이 되면 고위경영자 양성교육 후보군에 들어간다. 이는 미래 최고경영자(CEO) 후보들을 위한 핵심인재 교육으로 연 2회로 4주에 걸쳐 진행된다.이후 사장이 되면 매주 수요일 사장단회의에서 강연을 듣고 매년 말 다음해의 전략을 세우고 점검하는 1박2일간의 사장단 세미나에 참석해야 한다. 삼성그룹 관계자는 “인재 양성뿐 아니라 그룹의 핵심가치를 공유하고 전파하는 것도 교육”이라며 “삼성만의 DNA를 심기 위한 인재 관리의 핵심”이라고 말했다.', 'question': '지난달 신임 임원 교육에서 강연했던 사람의 이름은?', 'answers': [{'text': '김동재', 'answer_start': 175}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`split` 메소드를 이용하면 데이터 셋을 나눌 수 있다."
      ],
      "metadata": {
        "id": "MFEKcRsb1ORX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, dev_dataset = KoMRC.split(dataset)\n",
        "print(\"Number of Train Samples:\", len(train_dataset))\n",
        "print(\"Number of Dev Samples:\", len(dev_dataset))\n",
        "print(dev_dataset[0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "XgRCT3Sb1ORY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d96a3e-e74d-4e7e-de37-97a8670e4676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Train Samples: 10834\n",
            "Number of Dev Samples: 1203\n",
            "{'guid': '844e22ab28924c1697d5ac28801b34c1', 'context': '지난해 주요 연극상을 나눠 가졌던 세 편의 작품이 올봄에 나란히 앙코르 무대를 갖는다. 대한민국연극대상 연기·무대예술상, 동아연극상 작품·희곡·연기상 등을 수상한 ‘알리바이 연대기’(17~20일 대학로 아르코예술극장 대극장, 25일~5월11일 서계동 국립극단 백성희장민호극장), 연극대상에서 대상과 희곡상을 받은 ‘여기가 집이다’(18일~5월22일 대학로 연우소극장), 연극대상 작품·연출상과 김상열연극상 수상작인 ‘황금용’(5월9~18일 서강대 메리홀 대극장)이다. 초연 당시 짧은 상연 기간과 낮은 인지도 등으로 공연을 놓친 연극팬에겐 평단으로부터 작품성을 인정받은 수작을 관람할 수 있는 기회다. ‘알리바이 연대기’는 희곡을 쓰고 연출한 김재엽의 가족사에 근거한 다큐멘터리 드라마다. 1930년에 태어난 한 개인의 사적인 연대기를 바탕으로 그 사이를 파고드는 역사적 순간들을 정밀하게 조명한다. 연출가는 “공적인 권력이 사적인 권리를 지켜주기보다 억압하기 일쑤였던 한국 현대사 속에서 개인은 언제나 무죄를 입증하며 하루하루 자신을 지켜내야 하는 ‘알리바이의 연대기’ 속에서 살아왔다”고 말한다.한국연극평론가협회는 이 작품을 ‘2013년 올해의 연극 베스트3’로 선정하며 “촘촘하고 세세하게 삶에 천착해 개인과 역사에 대한 이분법적 관점을 극복한다. 정치극에 대한 새로운 가능성을 보여줬다”고 평했다. 이 작품으로 연기상을 휩쓴 남명렬을 비롯해 지춘성 정원조 등 초연 배우들이 그대로 출연한다.‘여기가 집이다’는 허름하고 볼품 없는 ‘20년 전통’의 고시원에 모여 사는 사람들의 절망과 희망을 그린 작품. ‘차력사와 아코디언’ ‘택배 왔어요’를 만든 극단 이와삼의 장우재 대표가 직접 대본을 쓰고 연출했다. 나름의 규칙을 가지고 평화로웠던 고시원에 새로운 주인으로 등장한 ‘20세 고등학생’ 동교가 “이제부터 고시원 식구들에게 월세를 받지 않겠다”고 선언하면서 갑작스런 변화의 바람이 분다.날것 그대로의 직설 화법으로 풀어 놓는 풍성한 인생 이야기와 생동감 넘치는 극적 구조로 ‘집’의 본원적 의미와 삶에 대한 성찰의 기회를 제공한다는 평가를 받았다. 재연에서는 중견 배우 김세동이 장씨 역으로 출연해 박무영 김충근 한동규 류제승 김정민 등 초연 배우들과 호흡을 맞춘다.독일 극작가 롤란트 시멜페니히가 쓴 현대극 ‘황금용’은 독일 소도시에 있는 아시아계 간이식당을 배경으로 현대 물질사회와 세계화 속에 가려진 욕망과 폭력, 소외를 그린다. 치통을 앓지만 불법 체류자 신분으로 치과에 가지 못하는 한 젊은 중국인 요리사는 결국 비참한 최후를 맞는다.작품을 연출한 윤광진 용인대 교수는 “극의 배경은 유럽의 한 소도시이지만 서울이나 경기 안산의 어느 거리에서 일어나는 듯 우리에게 가깝게 다가오는 작품”이라며 “지하철에서 마주치는 외국인 근로자들, 그 옆에서 졸고 있는 우리의 이야기”라고 말했다. 이호성 남미정 이동근 한덕호 방현숙 등 초연 배우들이 다시 뭉친다.', 'question': '윤광진 교수가 연출한 연극이 앙코르 공연을 하는 장소는 어디인가?', 'answers': [{'text': '서강대 메리홀 대극장', 'answer_start': 246}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 단위로 토큰화해서 정답 위치를 찾기 위하여 토큰화 및 위치 인덱싱을 하는 클래스를 상속을 통해 작성해 보자."
      ],
      "metadata": {
        "id": "_EgKh3Nn1ORY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Generator\n",
        "\n",
        "import konlpy\n",
        "\n",
        "class TokenizedKoMRC(KoMRC):\n",
        "    def __init__(self, data, indices: List[Tuple[int, int, int]]) -> None:\n",
        "        super().__init__(data, indices)\n",
        "        self._tagger = konlpy.tag.Mecab()\n",
        "\n",
        "    def _tokenize_with_position(self, sentence: str) -> List[Tuple[str, Tuple[int, int]]]:\n",
        "        position = 0\n",
        "        tokens = []\n",
        "        for morph in self._tagger.morphs(sentence):\n",
        "            position = sentence.find(morph, position)\n",
        "            tokens.append((morph, (position, position + len(morph))))\n",
        "            position += len(morph)\n",
        "        return tokens\n",
        "            \n",
        "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
        "        sample = super().__getitem__(index)\n",
        "\n",
        "        context, position = zip(*self._tokenize_with_position(sample['context']))\n",
        "        context, position = list(context), list(position)\n",
        "        question = self._tagger.morphs(sample['question'])\n",
        "\n",
        "        if sample['answers'] is not None:\n",
        "            answers = []\n",
        "            for answer in sample['answers']:\n",
        "                for start, (position_start, position_end) in enumerate(position):\n",
        "                    if position_start <= answer['answer_start'] < position_end:\n",
        "                        break\n",
        "                else:\n",
        "                    print(context, answer)\n",
        "                    raise ValueError(\"No mathced start position\")\n",
        "\n",
        "                target = ''.join(answer['text'].split(' '))\n",
        "                source = ''\n",
        "                for end, morph in enumerate(context[start:], start):\n",
        "                    source += morph\n",
        "                    if target in source:\n",
        "                        break\n",
        "                else:\n",
        "                    print(context, answer)\n",
        "                    raise ValueError(\"No Matched end position\")\n",
        "\n",
        "                answers.append({\n",
        "                    'start': start,\n",
        "                    'end': end\n",
        "                })\n",
        "        else:\n",
        "            answers = None\n",
        "        \n",
        "        return {\n",
        "            'guid': sample['guid'],\n",
        "            'context_original': sample['context'],\n",
        "            'context_position': position,\n",
        "            'question_original': sample['question'],\n",
        "            'context': context,\n",
        "            'question': question,\n",
        "            'answers': answers\n",
        "        }\n",
        "        "
      ],
      "metadata": {
        "trusted": true,
        "id": "74kDJppW1ORY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TokenizedKoMRC.load('/content/drive/MyDrive/Goorm/project02/data/train.json')\n",
        "\n",
        "train_dataset, dev_dataset = TokenizedKoMRC.split(dataset)\n",
        "print(\"Number of Train Samples:\", len(train_dataset))\n",
        "print(\"Number of Dev Samples:\", len(dev_dataset))\n",
        "print(dev_dataset[0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "OGkydRBd1ORZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b931f65-b456-4196-bd71-90346fc3bfae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Train Samples: 10834\n",
            "Number of Dev Samples: 1203\n",
            "{'guid': '844e22ab28924c1697d5ac28801b34c1', 'context_original': '지난해 주요 연극상을 나눠 가졌던 세 편의 작품이 올봄에 나란히 앙코르 무대를 갖는다. 대한민국연극대상 연기·무대예술상, 동아연극상 작품·희곡·연기상 등을 수상한 ‘알리바이 연대기’(17~20일 대학로 아르코예술극장 대극장, 25일~5월11일 서계동 국립극단 백성희장민호극장), 연극대상에서 대상과 희곡상을 받은 ‘여기가 집이다’(18일~5월22일 대학로 연우소극장), 연극대상 작품·연출상과 김상열연극상 수상작인 ‘황금용’(5월9~18일 서강대 메리홀 대극장)이다. 초연 당시 짧은 상연 기간과 낮은 인지도 등으로 공연을 놓친 연극팬에겐 평단으로부터 작품성을 인정받은 수작을 관람할 수 있는 기회다. ‘알리바이 연대기’는 희곡을 쓰고 연출한 김재엽의 가족사에 근거한 다큐멘터리 드라마다. 1930년에 태어난 한 개인의 사적인 연대기를 바탕으로 그 사이를 파고드는 역사적 순간들을 정밀하게 조명한다. 연출가는 “공적인 권력이 사적인 권리를 지켜주기보다 억압하기 일쑤였던 한국 현대사 속에서 개인은 언제나 무죄를 입증하며 하루하루 자신을 지켜내야 하는 ‘알리바이의 연대기’ 속에서 살아왔다”고 말한다.한국연극평론가협회는 이 작품을 ‘2013년 올해의 연극 베스트3’로 선정하며 “촘촘하고 세세하게 삶에 천착해 개인과 역사에 대한 이분법적 관점을 극복한다. 정치극에 대한 새로운 가능성을 보여줬다”고 평했다. 이 작품으로 연기상을 휩쓴 남명렬을 비롯해 지춘성 정원조 등 초연 배우들이 그대로 출연한다.‘여기가 집이다’는 허름하고 볼품 없는 ‘20년 전통’의 고시원에 모여 사는 사람들의 절망과 희망을 그린 작품. ‘차력사와 아코디언’ ‘택배 왔어요’를 만든 극단 이와삼의 장우재 대표가 직접 대본을 쓰고 연출했다. 나름의 규칙을 가지고 평화로웠던 고시원에 새로운 주인으로 등장한 ‘20세 고등학생’ 동교가 “이제부터 고시원 식구들에게 월세를 받지 않겠다”고 선언하면서 갑작스런 변화의 바람이 분다.날것 그대로의 직설 화법으로 풀어 놓는 풍성한 인생 이야기와 생동감 넘치는 극적 구조로 ‘집’의 본원적 의미와 삶에 대한 성찰의 기회를 제공한다는 평가를 받았다. 재연에서는 중견 배우 김세동이 장씨 역으로 출연해 박무영 김충근 한동규 류제승 김정민 등 초연 배우들과 호흡을 맞춘다.독일 극작가 롤란트 시멜페니히가 쓴 현대극 ‘황금용’은 독일 소도시에 있는 아시아계 간이식당을 배경으로 현대 물질사회와 세계화 속에 가려진 욕망과 폭력, 소외를 그린다. 치통을 앓지만 불법 체류자 신분으로 치과에 가지 못하는 한 젊은 중국인 요리사는 결국 비참한 최후를 맞는다.작품을 연출한 윤광진 용인대 교수는 “극의 배경은 유럽의 한 소도시이지만 서울이나 경기 안산의 어느 거리에서 일어나는 듯 우리에게 가깝게 다가오는 작품”이라며 “지하철에서 마주치는 외국인 근로자들, 그 옆에서 졸고 있는 우리의 이야기”라고 말했다. 이호성 남미정 이동근 한덕호 방현숙 등 초연 배우들이 다시 뭉친다.', 'context_position': [(0, 3), (4, 6), (7, 9), (9, 10), (10, 11), (12, 14), (15, 17), (17, 18), (19, 20), (21, 22), (22, 23), (24, 26), (26, 27), (28, 29), (29, 30), (30, 31), (32, 35), (36, 39), (40, 42), (42, 43), (44, 45), (45, 47), (47, 48), (49, 53), (53, 55), (55, 57), (58, 60), (60, 61), (61, 63), (63, 65), (65, 66), (66, 67), (68, 70), (70, 72), (72, 73), (74, 76), (76, 77), (77, 79), (79, 80), (80, 82), (82, 83), (84, 85), (85, 86), (87, 89), (89, 90), (91, 92), (92, 96), (97, 100), (100, 102), (102, 104), (104, 105), (105, 107), (107, 108), (109, 111), (111, 112), (113, 116), (116, 118), (118, 120), (121, 124), (124, 125), (126, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 134), (134, 135), (136, 139), (140, 144), (145, 148), (148, 151), (151, 153), (153, 154), (154, 155), (156, 158), (158, 160), (160, 162), (163, 165), (165, 166), (167, 168), (168, 170), (170, 171), (172, 173), (173, 174), (175, 176), (176, 178), (178, 179), (180, 181), (181, 182), (182, 183), (183, 185), (185, 187), (187, 188), (188, 189), (189, 190), (190, 191), (191, 193), (193, 194), (195, 197), (197, 198), (199, 201), (201, 204), (204, 205), (205, 206), (207, 209), (209, 211), (212, 214), (214, 215), (215, 217), (217, 218), (218, 219), (220, 223), (223, 225), (225, 226), (227, 229), (229, 231), (232, 233), (233, 236), (236, 238), (238, 239), (239, 240), (240, 241), (241, 242), (242, 244), (244, 245), (246, 249), (250, 252), (252, 253), (254, 257), (257, 258), (258, 259), (259, 260), (260, 261), (262, 264), (265, 267), (268, 269), (269, 270), (271, 273), (274, 276), (276, 277), (278, 279), (279, 280), (281, 284), (285, 286), (286, 288), (289, 291), (291, 292), (293, 295), (296, 298), (298, 299), (299, 301), (302, 304), (304, 308), (309, 311), (311, 312), (312, 313), (314, 317), (317, 318), (319, 321), (321, 322), (323, 325), (325, 326), (327, 328), (329, 330), (330, 331), (332, 334), (334, 335), (335, 336), (337, 338), (338, 342), (343, 346), (346, 347), (347, 348), (349, 351), (351, 352), (353, 354), (354, 355), (356, 358), (358, 359), (360, 363), (363, 364), (365, 368), (368, 369), (370, 372), (372, 373), (374, 379), (380, 383), (383, 384), (384, 385), (386, 390), (390, 391), (391, 392), (393, 396), (397, 398), (399, 401), (401, 402), (403, 404), (404, 405), (405, 406), (407, 410), (410, 411), (412, 414), (414, 416), (417, 418), (419, 421), (421, 422), (423, 426), (426, 427), (428, 430), (430, 431), (432, 434), (434, 435), (435, 436), (437, 439), (439, 440), (440, 441), (442, 444), (444, 446), (446, 447), (448, 451), (451, 452), (453, 454), (454, 455), (455, 456), (456, 457), (458, 460), (460, 461), (462, 463), (463, 464), (464, 465), (466, 468), (468, 469), (470, 472), (472, 473), (473, 474), (474, 476), (477, 479), (479, 480), (480, 481), (482, 484), (484, 485), (485, 486), (487, 489), (490, 493), (494, 495), (495, 497), (498, 500), (500, 501), (502, 505), (506, 508), (508, 509), (510, 512), (512, 513), (513, 514), (515, 519), (520, 522), (522, 523), (524, 526), (526, 527), (527, 528), (529, 530), (530, 531), (532, 533), (533, 537), (537, 538), (539, 542), (542, 543), (544, 545), (545, 547), (548, 551), (551, 552), (552, 553), (553, 554), (555, 556), (556, 558), (558, 559), (559, 561), (561, 563), (563, 566), (566, 568), (568, 569), (570, 571), (572, 574), (574, 575), (576, 577), (577, 581), (581, 582), (583, 585), (585, 586), (587, 589), (590, 593), (593, 594), (594, 595), (595, 596), (597, 599), (599, 600), (600, 601), (602, 603), (603, 605), (605, 606), (606, 607), (608, 610), (610, 611), (611, 612), (613, 614), (614, 615), (616, 618), (618, 619), (620, 622), (622, 623), (624, 626), (626, 627), (628, 630), (631, 634), (634, 635), (636, 638), (638, 639), (640, 642), (642, 644), (644, 645), (646, 648), (648, 649), (649, 650), (651, 653), (654, 657), (658, 660), (660, 661), (661, 662), (663, 665), (665, 666), (666, 667), (667, 668), (668, 669), (670, 672), (672, 673), (673, 674), (675, 676), (677, 679), (679, 681), (682, 684), (684, 685), (685, 686), (687, 689), (690, 693), (693, 694), (695, 697), (697, 698), (699, 702), (703, 706), (707, 708), (709, 711), (712, 714), (714, 715), (715, 716), (717, 720), (721, 723), (723, 725), (725, 726), (726, 727), (727, 729), (729, 730), (731, 732), (732, 733), (733, 734), (734, 735), (735, 736), (737, 739), (739, 740), (740, 741), (742, 744), (745, 746), (746, 747), (748, 749), (749, 751), (751, 752), (753, 755), (755, 756), (756, 757), (758, 761), (761, 762), (763, 765), (766, 767), (767, 768), (769, 771), (771, 772), (772, 773), (774, 776), (776, 777), (778, 780), (780, 781), (782, 784), (785, 787), (787, 788), (789, 790), (790, 791), (791, 793), (793, 794), (795, 799), (799, 800), (801, 802), (802, 804), (805, 806), (806, 808), (808, 809), (809, 810), (811, 813), (814, 816), (817, 818), (818, 819), (819, 820), (820, 821), (822, 825), (826, 828), (828, 829), (830, 832), (833, 835), (835, 836), (837, 838), (838, 839), (840, 842), (842, 843), (843, 844), (844, 845), (846, 848), (848, 849), (850, 852), (852, 853), (854, 856), (856, 857), (858, 860), (860, 862), (862, 863), (864, 867), (867, 868), (869, 872), (873, 875), (875, 877), (878, 880), (880, 881), (882, 883), (883, 885), (885, 886), (887, 891), (891, 892), (893, 895), (895, 896), (897, 898), (898, 900), (900, 902), (903, 906), (907, 909), (909, 910), (910, 912), (913, 915), (915, 916), (917, 918), (918, 919), (920, 921), (921, 922), (922, 923), (923, 924), (924, 925), (926, 928), (928, 929), (929, 931), (932, 936), (937, 939), (939, 940), (941, 943), (943, 944), (945, 947), (947, 948), (948, 949), (949, 950), (951, 954), (954, 955), (956, 958), (959, 961), (961, 963), (964, 965), (965, 966), (967, 968), (968, 969), (970, 972), (972, 973), (974, 976), (977, 980), (980, 981), (982, 985), (986, 988), (988, 989), (990, 991), (991, 992), (993, 995), (995, 996), (997, 998), (998, 999), (999, 1000), (1000, 1001), (1002, 1004), (1004, 1005), (1006, 1008), (1008, 1009), (1010, 1011), (1011, 1012), (1013, 1015), (1016, 1018), (1018, 1019), (1020, 1022), (1022, 1023), (1024, 1026), (1026, 1029), (1030, 1032), (1032, 1033), (1034, 1035), (1035, 1036), (1036, 1037), (1037, 1038), (1039, 1041), (1041, 1043), (1043, 1044), (1045, 1047), (1048, 1050), (1051, 1054), (1054, 1055), (1056, 1057), (1057, 1058), (1059, 1060), (1060, 1062), (1063, 1065), (1065, 1066), (1067, 1070), (1071, 1074), (1075, 1078), (1079, 1082), (1083, 1086), (1087, 1088), (1089, 1091), (1092, 1094), (1094, 1095), (1095, 1096), (1097, 1099), (1099, 1100), (1101, 1104), (1104, 1105), (1105, 1107), (1108, 1111), (1112, 1115), (1116, 1118), (1118, 1121), (1121, 1122), (1123, 1124), (1125, 1128), (1129, 1130), (1130, 1133), (1133, 1134), (1134, 1135), (1136, 1138), (1139, 1140), (1140, 1142), (1142, 1143), (1144, 1145), (1145, 1146), (1147, 1150), (1150, 1151), (1152, 1156), (1156, 1157), (1158, 1160), (1160, 1162), (1163, 1165), (1166, 1168), (1168, 1170), (1170, 1171), (1172, 1174), (1174, 1175), (1176, 1177), (1177, 1178), (1179, 1182), (1183, 1185), (1185, 1186), (1187, 1189), (1189, 1190), (1191, 1193), (1193, 1194), (1195, 1198), (1198, 1199), (1200, 1202), (1202, 1203), (1204, 1205), (1205, 1207), (1208, 1210), (1211, 1213), (1213, 1214), (1215, 1217), (1217, 1219), (1220, 1222), (1222, 1223), (1224, 1225), (1225, 1226), (1227, 1229), (1229, 1230), (1231, 1232), (1233, 1234), (1234, 1235), (1236, 1239), (1240, 1243), (1243, 1244), (1245, 1247), (1248, 1250), (1250, 1251), (1252, 1254), (1254, 1255), (1256, 1257), (1257, 1259), (1259, 1260), (1260, 1262), (1262, 1263), (1264, 1266), (1266, 1267), (1268, 1271), (1272, 1274), (1274, 1275), (1276, 1278), (1278, 1279), (1280, 1281), (1281, 1282), (1282, 1283), (1284, 1286), (1286, 1287), (1288, 1290), (1290, 1291), (1292, 1293), (1294, 1297), (1297, 1298), (1298, 1300), (1301, 1303), (1303, 1305), (1306, 1308), (1309, 1311), (1311, 1312), (1313, 1315), (1316, 1318), (1318, 1320), (1321, 1324), (1324, 1325), (1326, 1327), (1328, 1330), (1330, 1332), (1333, 1335), (1335, 1336), (1337, 1340), (1340, 1341), (1342, 1344), (1344, 1345), (1345, 1346), (1346, 1348), (1349, 1350), (1350, 1353), (1353, 1355), (1356, 1359), (1359, 1360), (1361, 1364), (1365, 1368), (1368, 1369), (1369, 1370), (1371, 1372), (1373, 1374), (1374, 1376), (1377, 1378), (1378, 1379), (1380, 1381), (1381, 1382), (1383, 1385), (1385, 1386), (1387, 1390), (1390, 1391), (1391, 1393), (1394, 1395), (1395, 1396), (1396, 1397), (1397, 1398), (1399, 1402), (1403, 1406), (1407, 1410), (1411, 1414), (1415, 1416), (1416, 1418), (1419, 1420), (1421, 1423), (1424, 1426), (1426, 1427), (1427, 1428), (1429, 1431), (1432, 1435), (1435, 1436)], 'question_original': '윤광진 교수가 연출한 연극이 앙코르 공연을 하는 장소는 어디인가?', 'context': ['지난해', '주요', '연극', '상', '을', '나눠', '가졌', '던', '세', '편', '의', '작품', '이', '올', '봄', '에', '나란히', '앙코르', '무대', '를', '갖', '는다', '.', '대한민국', '연극', '대상', '연기', '·', '무대', '예술', '상', ',', '동아', '연극', '상', '작품', '·', '희곡', '·', '연기', '상', '등', '을', '수상', '한', '‘', '알리바이', '연대기', '’(', '17', '~', '20', '일', '대학', '로', '아르코', '예술', '극장', '대극장', ',', '25', '일', '~', '5', '월', '11', '일', '서계동', '국립극단', '백성희', '장민호', '극장', ')', ',', '연극', '대상', '에서', '대상', '과', '희', '곡상', '을', '받', '은', '‘', '여기', '가', '집', '이', '다', '’(', '18', '일', '~', '5', '월', '22', '일', '대학', '로', '연우', '소극장', ')', ',', '연극', '대상', '작품', '·', '연출', '상', '과', '김상열', '연극', '상', '수상', '작인', '‘', '황금용', '’(', '5', '월', '9', '~', '18', '일', '서강대', '메리', '홀', '대극장', ')', '이', '다', '.', '초연', '당시', '짧', '은', '상연', '기간', '과', '낮', '은', '인지도', '등', '으로', '공연', '을', '놓친', '연극', '팬', '에겐', '평단', '으로부터', '작품', '성', '을', '인정받', '은', '수작', '을', '관람', '할', '수', '있', '는', '기회', '다', '.', '‘', '알리바이', '연대기', '’', '는', '희곡', '을', '쓰', '고', '연출', '한', '김재엽', '의', '가족사', '에', '근거', '한', '다큐멘터리', '드라마', '다', '.', '1930', '년', '에', '태어난', '한', '개인', '의', '사', '적', '인', '연대기', '를', '바탕', '으로', '그', '사이', '를', '파고드', '는', '역사', '적', '순간', '들', '을', '정밀', '하', '게', '조명', '한다', '.', '연출가', '는', '“', '공', '적', '인', '권력', '이', '사', '적', '인', '권리', '를', '지켜', '주', '기', '보다', '억압', '하', '기', '일쑤', '였', '던', '한국', '현대사', '속', '에서', '개인', '은', '언제나', '무죄', '를', '입증', '하', '며', '하루하루', '자신', '을', '지켜', '내', '야', '하', '는', '‘', '알리바이', '의', '연대기', '’', '속', '에서', '살아왔', '다', '”', '고', '말', '한다', '.', '한국', '연극', '평론가', '협회', '는', '이', '작품', '을', '‘', '2013', '년', '올해', '의', '연극', '베스트', '3', '’', '로', '선정', '하', '며', '“', '촘촘', '하', '고', '세세', '하', '게', '삶', '에', '천착', '해', '개인', '과', '역사', '에', '대한', '이분법', '적', '관점', '을', '극복', '한다', '.', '정치', '극', '에', '대한', '새로운', '가능', '성', '을', '보여', '줬', '다', '”', '고', '평했', '다', '.', '이', '작품', '으로', '연기', '상', '을', '휩쓴', '남명렬', '을', '비롯', '해', '지춘성', '정원조', '등', '초연', '배우', '들', '이', '그대로', '출연', '한다', '.', '‘', '여기', '가', '집', '이', '다', '’', '는', '허름', '하', '고', '볼품', '없', '는', '‘', '20', '년', '전통', '’', '의', '고시원', '에', '모여', '사', '는', '사람', '들', '의', '절망', '과', '희망', '을', '그린', '작품', '.', '‘', '차', '력사', '와', '아코디언', '’', '‘', '택배', '왔', '어요', '’', '를', '만든', '극단', '이', '와', '삼', '의', '장우재', '대표', '가', '직접', '대본', '을', '쓰', '고', '연출', '했', '다', '.', '나름', '의', '규칙', '을', '가지', '고', '평화', '로웠', '던', '고시원', '에', '새로운', '주인', '으로', '등장', '한', '‘', '20', '세', '고등학생', '’', '동교', '가', '“', '이제', '부터', '고시원', '식구', '들', '에게', '월세', '를', '받', '지', '않', '겠', '다', '”', '고', '선언', '하', '면서', '갑작스런', '변화', '의', '바람', '이', '분다', '.', '날', '것', '그대로', '의', '직설', '화법', '으로', '풀', '어', '놓', '는', '풍성', '한', '인생', '이야기', '와', '생동감', '넘치', '는', '극', '적', '구조', '로', '‘', '집', '’', '의', '본원', '적', '의미', '와', '삶', '에', '대한', '성찰', '의', '기회', '를', '제공', '한다는', '평가', '를', '받', '았', '다', '.', '재연', '에서', '는', '중견', '배우', '김세동', '이', '장', '씨', '역', '으로', '출연', '해', '박무영', '김충근', '한동규', '류제승', '김정민', '등', '초연', '배우', '들', '과', '호흡', '을', '맞춘다', '.', '독일', '극작가', '롤란트', '시멜', '페니히', '가', '쓴', '현대극', '‘', '황금용', '’', '은', '독일', '소', '도시', '에', '있', '는', '아시아', '계', '간이식당', '을', '배경', '으로', '현대', '물질', '사회', '와', '세계', '화', '속', '에', '가려진', '욕망', '과', '폭력', ',', '소외', '를', '그린다', '.', '치통', '을', '앓', '지만', '불법', '체류', '자', '신분', '으로', '치과', '에', '가', '지', '못하', '는', '한', '젊', '은', '중국인', '요리사', '는', '결국', '비참', '한', '최후', '를', '맞', '는다', '.', '작품', '을', '연출', '한', '윤광진', '용인', '대', '교수', '는', '“', '극', '의', '배경', '은', '유럽', '의', '한', '소도시', '이', '지만', '서울', '이나', '경기', '안산', '의', '어느', '거리', '에서', '일어나', '는', '듯', '우리', '에게', '가깝', '게', '다가오', '는', '작품', '”', '이', '라며', '“', '지하철', '에서', '마주치', '는', '외국인', '근로자', '들', ',', '그', '옆', '에서', '졸', '고', '있', '는', '우리', '의', '이야기', '”', '라고', '말', '했', '다', '.', '이호성', '남미정', '이동근', '한덕호', '방', '현숙', '등', '초연', '배우', '들', '이', '다시', '뭉친다', '.'], 'question': ['윤광진', '교수', '가', '연출', '한', '연극', '이', '앙코르', '공연', '을', '하', '는', '장소', '는', '어디', '인가', '?'], 'answers': [{'start': 125, 'end': 128}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dev_dataset[0]\n",
        "print(sample['context'][sample['answers'][0]['start']:sample['answers'][0]['end']+1])"
      ],
      "metadata": {
        "trusted": true,
        "id": "GV0GWgE91ORa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd98bf5-7a86-4a34-8f01-17637bd44b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['서강대', '메리', '홀', '대극장']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocab 생성 및 Indexing\n",
        "토큰화된 데이터 셋을 기준으로 Vocab을 만들고 인덱싱을 하는 `Indexer`를 만들자."
      ],
      "metadata": {
        "id": "0reClxVI1ORa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class Indexer:\n",
        "    def __init__(self,\n",
        "        id2token: List[str], \n",
        "        max_length: int=1024,\n",
        "        pad: str='<pad>', unk: str='<unk>', cls: str='<cls>', sep: str='<sep>'\n",
        "    ):\n",
        "        self.pad = pad\n",
        "        self.unk = unk\n",
        "        self.cls = cls\n",
        "        self.sep = sep\n",
        "        self.special_tokens = [pad, unk, cls, sep]\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.id2token = self.special_tokens + id2token\n",
        "        self.token2id = {token: token_id for token_id, token in enumerate(self.id2token)}\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.id2token)\n",
        "    \n",
        "    @property\n",
        "    def pad_id(self):\n",
        "        return self.token2id[self.pad]\n",
        "    @property\n",
        "    def unk_id(self):\n",
        "        return self.token2id[self.unk]\n",
        "    @property\n",
        "    def cls_id(self):\n",
        "        return self.token2id[self.cls]\n",
        "    @property\n",
        "    def sep_id(self):\n",
        "        return self.token2id[self.sep]\n",
        "\n",
        "    @classmethod\n",
        "    def build_vocab(cls,\n",
        "        dataset: TokenizedKoMRC, \n",
        "        min_freq: int=5\n",
        "    ):\n",
        "        counter = Counter(chain.from_iterable(\n",
        "            sample['context'] + sample['question']\n",
        "            for sample in tqdm(dataset, desc=\"Counting Vocab\")\n",
        "        ))\n",
        "\n",
        "        return cls([word for word, count in counter.items() if count >= min_freq])\n",
        "    \n",
        "    def decode(self,\n",
        "        token_ids: Sequence[int]\n",
        "    ):\n",
        "        return [self.id2token[token_id] for token_id in token_ids]\n",
        "\n",
        "    def sample2ids(self,\n",
        "        sample: Dict[str, Any],\n",
        "    ) -> Dict[str, Any]:\n",
        "        context = [self.token2id.get(token, self.unk_id) for token in sample['context']]\n",
        "        question = [self.token2id.get(token, self.unk_id) for token in sample['question']]\n",
        "\n",
        "        context = context[:self.max_length-len(question)-3]             # Truncate context\n",
        "        \n",
        "        input_ids = [self.cls_id] + question + [self.sep_id] + context + [self.sep_id]\n",
        "        token_type_ids = [0] * (len(question) + 1) + [1] * (len(context) + 2)\n",
        "\n",
        "        if sample['answers'] is not None:\n",
        "            answer = sample['answers'][0]\n",
        "            start = min(answer['start'] + len(question) + 2, self.max_length - 1)\n",
        "            end = min(answer['end'] + len(question) + 2, self.max_length - 1)\n",
        "        else:\n",
        "            start = None\n",
        "            end = None\n",
        "\n",
        "        return {\n",
        "            'guid': sample['guid'],\n",
        "            'context': sample['context_original'],\n",
        "            'question': sample['question_original'],\n",
        "            'position': sample['context_position'],\n",
        "            'input_ids': input_ids,\n",
        "            'token_type_ids': token_type_ids,\n",
        "            'start': start,\n",
        "            'end': end\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "id": "w-i8-hFY1ORa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexer = Indexer.build_vocab(dataset)\n",
        "print(indexer.sample2ids(dev_dataset[0]))"
      ],
      "metadata": {
        "trusted": true,
        "id": "19dHOfMO1ORb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "쉽게 Indexer를 활용하기 위해 Indexer가 포함된 데이터 셋을 만들자."
      ],
      "metadata": {
        "id": "LupABrtX1ORb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IndexerWrappedDataset:\n",
        "    def __init__(self, dataset: TokenizedKoMRC, indexer: Indexer) -> None:\n",
        "        self._dataset = dataset\n",
        "        self._indexer = indexer\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._dataset)\n",
        "    \n",
        "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
        "        sample = self._indexer.sample2ids(self._dataset[index])\n",
        "        sample['attention_mask'] = [1] * len(sample['input_ids'])\n",
        "\n",
        "        return sample\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "W4QRMpMp1ORb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_train_dataset = IndexerWrappedDataset(train_dataset, indexer)\n",
        "indexed_dev_dataset = IndexerWrappedDataset(dev_dataset, indexer)\n",
        "\n",
        "sample = indexed_dev_dataset[0]\n",
        "print(sample['input_ids'], sample['attention_mask'], sample['token_type_ids'], sample['start'], sample['end'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "80dNUSFy1ORc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder를 활용한 MRC 모델\n",
        "![Bert for MRC](https://miro.medium.com/max/340/1*cXDOP0gsE7Zp8-sgZqYfTA.png)\n",
        "\n",
        "Transformer 인코더 마지막에 Linear Layer를 붙여 정답의 시작과 끝을 맞추는 간단한 모델을 생성보자."
      ],
      "metadata": {
        "id": "3luB32EC1ORc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "from transformers.models.bert.modeling_bert import (\n",
        "    BertModel,\n",
        "    BertPreTrainedModel\n",
        ")\n",
        "\n",
        "## Simple Version for Bert QA: https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForQuestionAnswering.forward\n",
        "class BertForQuestionAnswering(BertPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.start_linear = nn.Linear(config.hidden_size, 1)\n",
        "        self.end_linear = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )\n",
        "\n",
        "        start_logits = self.start_linear(outputs.last_hidden_state).squeeze(-1)\n",
        "        end_logits = self.end_linear(outputs.last_hidden_state).squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "metadata": {
        "trusted": true,
        "id": "_dffNvQL1ORc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 준비"
      ],
      "metadata": {
        "id": "SpXfK5Wx1ORd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class Collator:\n",
        "    def __init__(self, indexer: Indexer) -> None:\n",
        "        self._indexer = indexer\n",
        "\n",
        "    def __call__(self, samples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        samples = {\n",
        "            key: [sample[key] for sample in samples]\n",
        "            for key in samples[0]\n",
        "        }\n",
        "\n",
        "        for key in 'start', 'end':\n",
        "            if samples[key][0] is None:\n",
        "                samples[key] = None\n",
        "            else:\n",
        "                samples[key] = torch.tensor(samples[key], dtype=torch.long)\n",
        "        for key in 'input_ids', 'attention_mask', 'token_type_ids':\n",
        "            samples[key] = pad_sequence(\n",
        "                [torch.tensor(sample, dtype=torch.long) for sample in samples[key]],\n",
        "                batch_first=True, padding_value=self._indexer.pad_id\n",
        "            )\n",
        "\n",
        "        return samples"
      ],
      "metadata": {
        "trusted": true,
        "id": "FB3lbayn1ORd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "accumulation = 4 # 메모리를 아끼기 위하여 Gradient accumulation을 해보자\n",
        "\n",
        "collator = Collator(indexer)\n",
        "train_loader = DataLoader(indexed_train_dataset, batch_size=batch_size//accumulation, shuffle=True, collate_fn=collator, num_workers=2)\n",
        "dev_loader = DataLoader(indexed_dev_dataset, batch_size=batch_size//accumulation, shuffle=False, collate_fn=collator, num_workers=2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "sLRCe2Mh1ORd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(dev_loader))\n",
        "print(batch['input_ids'].shape)\n",
        "print(batch['input_ids'])\n",
        "print(list(batch.keys()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "6Fnpk_rT1ORe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertConfig\n",
        "\n",
        "torch.manual_seed(42)\n",
        "config = BertConfig(\n",
        "     vocab_size=indexer.vocab_size,\n",
        "     max_position_embeddings=1024,\n",
        "     hidden_size=256,\n",
        "     num_hidden_layers=4,\n",
        "     num_attention_heads=4,\n",
        "     intermediate_size=1024\n",
        ")\n",
        "model = BertForQuestionAnswering(config)\n",
        "# model.cuda()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-P9Cr3wN1ORe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from statistics import mean\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "os.makedirs('dump', exist_ok=True)\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "\n",
        "step = 0\n",
        "\n",
        "for epoch in range(1, 31):\n",
        "    print(\"Epoch\", epoch)\n",
        "    # Training\n",
        "    running_loss = 0.\n",
        "    losses = []\n",
        "    progress_bar = tqdm(train_loader, desc='Train')\n",
        "    for batch in progress_bar:\n",
        "        del batch['guid'], batch['context'], batch['question'], batch['position']\n",
        "        # batch = {key: value.cuda() for key, value in batch.items()}\n",
        "        start = batch.pop('start')\n",
        "        end = batch.pop('end')\n",
        "        \n",
        "        start_logits, end_logits = model(**batch)\n",
        "        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
        "        (loss / accumulation).backward()\n",
        "        running_loss += loss.item()\n",
        "        del batch, start, end, start_logits, end_logits, loss\n",
        "        \n",
        "        step += 1\n",
        "        if step % accumulation:\n",
        "            continue\n",
        "\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        losses.append(running_loss / accumulation)\n",
        "        running_loss = 0.\n",
        "        progress_bar.set_description(f\"Train - Loss: {losses[-1]:.3f}\")\n",
        "    train_losses.append(mean(losses))\n",
        "    print(f\"train score: {train_losses[-1]:.3f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    losses = []\n",
        "    for batch in tqdm(dev_loader, desc=\"Evaluation\"):\n",
        "        del batch['guid'], batch['context'], batch['question'], batch['position']\n",
        "        # batch = {key: value.cuda() for key, value in batch.items()}\n",
        "        start = batch.pop('start')\n",
        "        end = batch.pop('end')\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            start_logits, end_logits = model(**batch)\n",
        "        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        del batch, start, end, start_logits, end_logits, loss\n",
        "    dev_losses.append(mean(losses))\n",
        "    print(f\"Evaluation score: {dev_losses[-1]:.3f}\")\n",
        "\n",
        "    model.save_pretrained(f'dump/model.{epoch}')"
      ],
      "metadata": {
        "trusted": true,
        "id": "Obq7jMtT1ORe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "t = list(range(1, 31))\n",
        "plt.plot(t, train_losses, label=\"Train Loss\")\n",
        "plt.plot(t, dev_losses, label=\"Dev Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "1V4AQwLx1ORf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![loss_plot](https://github.com/mynsng/mynsng.github.io/blob/master/assets/images/__results___26_0.png?raw=true)"
      ],
      "metadata": {
        "id": "vnvz79tf1ORf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 데이터 셋에 Overfitting이 일어나는 것을 확인할 수 있다."
      ],
      "metadata": {
        "id": "r3QMrCUk1ORf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer Inference\n",
        "모델의 Output을 활용해서 질문의 답을 찾는 코드를 작성하자."
      ],
      "metadata": {
        "id": "HPzr3c5a1ORf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('dump/model.30')\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "metadata": {
        "trusted": true,
        "id": "hp_OifiT1ORf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, sample in zip(range(1, 4), indexed_train_dataset):\n",
        "    print(f'------{idx}------')\n",
        "    print('Context:', sample['context'])\n",
        "    print('Question:', sample['question'])\n",
        "    \n",
        "    input_ids, token_type_ids = [\n",
        "        torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n",
        "        for key in (\"input_ids\", \"token_type_ids\")\n",
        "    ]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n",
        "    start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
        "    \n",
        "    start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n",
        "    end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n",
        "    probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n",
        "    index = torch.argmax(probability).item()\n",
        "    \n",
        "    start = index // len(end_prob)\n",
        "    end = index % len(end_prob)\n",
        "    \n",
        "    start = sample['position'][start][0]\n",
        "    end = sample['position'][end][1]\n",
        "\n",
        "    print('Answer:', sample['context'][start:end])"
      ],
      "metadata": {
        "trusted": true,
        "id": "wM8hhG-r1ORg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 출력 파일 작성"
      ],
      "metadata": {
        "id": "SvlxTDYV1ORg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TokenizedKoMRC.load('/content/drive/MyDrive/Goorm/project02/data/test.json')\n",
        "\n",
        "test_dataset = IndexerWrappedDataset(test_dataset, indexer)\n",
        "print(\"Number of Test Samples\", len(test_dataset))\n",
        "print(test_dataset[0])"
      ],
      "metadata": {
        "trusted": true,
        "id": "Bhl0itA01ORg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "os.makedirs('out', exist_ok=True)\n",
        "with torch.no_grad(), open('out/baseline.csv', 'w') as fd:\n",
        "    writer = csv.writer(fd)\n",
        "    writer.writerow(['Id', 'Predicted'])\n",
        "\n",
        "    rows = []\n",
        "    for sample in tqdm(test_dataset, \"Testing\"):\n",
        "        input_ids, token_type_ids = [\n",
        "            torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n",
        "            for key in (\"input_ids\", \"token_type_ids\")\n",
        "        ]\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n",
        "        start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
        "    \n",
        "        start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n",
        "        end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n",
        "        probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n",
        "        index = torch.argmax(probability).item()\n",
        "    \n",
        "        start = index // len(end_prob)\n",
        "        end = index % len(end_prob)\n",
        "    \n",
        "        start = sample['position'][start][0]\n",
        "        end = sample['position'][end][1]\n",
        "\n",
        "        rows.append([sample[\"guid\"], sample['context'][start:end]])\n",
        "    \n",
        "    writer.writerows(rows)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TDAQVHoK1ORg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNxXUkey1ORh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAhU691l1ORh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}